{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b5e2fb-7c82-4ed6-aa80-82471d5ec58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (356 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.9/356.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jellyfish\n",
      "Successfully installed jellyfish-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a03b091-79cd-40f1-9007-152b770e3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "from functools import reduce\n",
    "\n",
    "from itertools import combinations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, lit, col, concat_ws, struct, collect_list, expr, explode, monotonically_increasing_id, count\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from packages.utils.dataset_generation import MyDatasets\n",
    "from packages.utils.spark_udfs import soundex_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a1274-b622-4eaf-96b0-3a0d6b218cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================================\n",
    "# 1. Create a Spark session\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(cpu_cores)\n",
    "\n",
    "# Load Spark configuration from file\n",
    "config_path = os.path.join(project_root, \"packages\", \"conf\", \"spark.local.conf\")\n",
    "\n",
    "# Read configuration file and build SparkSession\n",
    "spark_builder = SparkSession.builder.master(\"local[*]\").appName(\"LocalTesting\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            spark_builder = spark_builder.config(key.strip(), value.strip())\n",
    "\n",
    "spark = spark_builder.getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "#     .appName(\"TungstenExample\")\\\n",
    "#     .config(\"spark.sql.tungsten.enabled\", \"true\")\\\n",
    "#     .config(\"spark.sql.codegen.wholeStage\", \"true\")\\\n",
    "#     .config(\"spark.default.parallelism\", \"200\")\\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", str(cpu_cores * 3))\\\n",
    "#     .config(\"spark.storage.memoryFraction\", \"0.4\")\\\n",
    "#     .config(\"spark.executor.memory\", \"4g\")\\\n",
    "#     .config(\"spark.driver.memory\", \"4g\")\\\n",
    "#     .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "#     .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\")\\\n",
    "#     .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\\\n",
    "#     .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\")\\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "#     .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "#     .config(\"spark.dynamicAllocation.maxExecutors\", \"10\")\\\n",
    "#     .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a702a0-7992-4532-b835-a94e878a705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "COLUMNS = [\"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\"]\n",
    "datasets = MyDatasets()\n",
    "dfs = datasets.size_1000().exclude([\"0\"]).soundex()\n",
    "# dfs = datasets.size_1000().exclude([\"0\"]).soundex().noise_50().hash_sha256()\n",
    "# dfs = datasets.size_10000()\n",
    "# dfs = datasets.size_50000()\n",
    "# dfs = datasets.size_75000()\n",
    "# dfs = datasets.size_100000().exclude([\"0\"]).soundex().noise_600().hash_sha256() # we call all the data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a697bd5-ced2-4979-aa28-92555a6c96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 2. Load data and preprocess them\n",
    "processed_dfs = []\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df = df[[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]]\n",
    "    df.columns = [\"_c0\", *COLUMNS]\n",
    "\n",
    "    df_tmp = spark.createDataFrame(df)\n",
    "    df_tmp = df_tmp.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_tmp = df_tmp.withColumn(\"origin\", lit(i))\n",
    "\n",
    "    # for c in COLUMNS:\n",
    "    #     df_tmp = df_tmp.withColumn(c, soundex_udf(df_tmp[c]))\n",
    "\n",
    "    processed_dfs.append(df_tmp.select('origin', 'id', '_c0', *COLUMNS))\n",
    "\n",
    "\n",
    "multiparty_datasets = processed_dfs[0]\n",
    "for df in processed_dfs[1:]:\n",
    "    multiparty_datasets = multiparty_datasets.union(df)\n",
    "\n",
    "# ===============================================================================================\n",
    "\n",
    "# ===============================================================================================\n",
    "# 3. Create the struct - The struct can be used for any combination of data request we give\n",
    "column_combinations = list(combinations(COLUMNS, 3))\n",
    "\n",
    "blocking_passes = []\n",
    "for pass_id, combo in enumerate(column_combinations):\n",
    "\n",
    "    df_with_block_key = multiparty_datasets.withColumn(\n",
    "        \"block_key\",\n",
    "        concat_ws(\"_\", *[col(c) for c in combo])\n",
    "    )\n",
    "\n",
    "    blocked_df = df_with_block_key.groupBy(\"block_key\").agg(\n",
    "        collect_list(struct(*['origin', 'id'])).alias(\"records\")\n",
    "    ).withColumn(\"pass_id\", lit(pass_id))\n",
    "\n",
    "    blocking_passes.append(blocked_df)\n",
    "\n",
    "multiparty_struct = reduce(lambda df1, df2: df1.unionByName(df2), blocking_passes).persist(StorageLevel.DISK_ONLY)\n",
    "if DEBUG : multiparty_struct.show(truncate=False)\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee9623-3405-4acb-a267-61b469a6a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 2. Load data and preprocess them\n",
    "processed_dfs = []\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df = df[[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "    df.columns = [\"_c0\", *COLUMNS]\n",
    "\n",
    "    df_tmp = spark.createDataFrame(df)\n",
    "    df_tmp = df_tmp.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_tmp = df_tmp.withColumn(\"origin\", lit(i))\n",
    "\n",
    "    processed_dfs.append(df_tmp.select('origin', 'id', '_c0', *COLUMNS))\n",
    "\n",
    "multiparty_datasets = processed_dfs[0]\n",
    "for df in processed_dfs[1:]:\n",
    "    multiparty_datasets = multiparty_datasets.union(df)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 3. Create the struct with greedy record assignment\n",
    "\n",
    "column_combinations = list(combinations(COLUMNS, 3))\n",
    "\n",
    "blocking_passes = []\n",
    "assigned_records = spark.createDataFrame([], schema=multiparty_datasets.select('origin', 'id').schema)  # empty tracker\n",
    "\n",
    "for pass_id, combo in enumerate(column_combinations):\n",
    "    # Keep only records not yet assigned\n",
    "    unassigned_records = multiparty_datasets.join(\n",
    "        assigned_records, on=['origin', 'id'], how='left_anti'\n",
    "    )\n",
    "\n",
    "    if unassigned_records.isEmpty():\n",
    "        print(f\"All records assigned by pass {pass_id - 1}\")\n",
    "        break  # Stop if no records remain\n",
    "\n",
    "    # Build block key\n",
    "    df_with_block_key = unassigned_records.withColumn(\n",
    "        \"block_key\",\n",
    "        concat_ws(\"_\", *[col(c) for c in combo])\n",
    "    )\n",
    "\n",
    "    blocked_df = df_with_block_key.groupBy(\"block_key\").agg(\n",
    "        collect_list(struct(*['origin', 'id'])).alias(\"records\"),\n",
    "        count(\"*\").alias(\"block_size\")\n",
    "    ).filter(col(\"block_size\") > 1).withColumn(\"pass_id\", lit(pass_id))\n",
    "\n",
    "    blocking_passes.append(blocked_df)\n",
    "\n",
    "    # Track assigned records\n",
    "    new_assigned = blocked_df.select(explode('records').alias('record')).select('record.origin', 'record.id')\n",
    "    assigned_records = assigned_records.union(new_assigned).dropDuplicates(['origin', 'id'])\n",
    "\n",
    "# Combine all passes\n",
    "if blocking_passes:\n",
    "    multiparty_struct = reduce(lambda df1, df2: df1.unionByName(df2), blocking_passes).persist(StorageLevel.DISK_ONLY)\n",
    "    if DEBUG: multiparty_struct.show(truncate=False)\n",
    "else:\n",
    "    multiparty_struct = spark.createDataFrame([], schema=multiparty_datasets.schema)\n",
    "\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff4023-8b98-4a60-8179-c3cab5678db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiparty_struct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bba772-a894-4af4-a448-3034f124eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 4. We have created our struct, we need to get the data now and we need to set the experiment.\n",
    "# If we want to get the matches of party 0 with all the rest of the matches we need to see \n",
    "# the configuration like this example\n",
    "\n",
    "to_party = 0\n",
    "from_parties = (1, 2, 3, 4)\n",
    "\n",
    "party = processed_dfs[to_party]\n",
    "\n",
    "# Union only the specified from_parties datasets\n",
    "ground_truth_datasets = processed_dfs[from_parties[0]]\n",
    "for i in from_parties[1:]:\n",
    "    ground_truth_datasets = ground_truth_datasets.union(processed_dfs[i])\n",
    "\n",
    "# Perform join and count\n",
    "ground_truth = party.select('_c0').join(\n",
    "    ground_truth_datasets\n",
    "        .filter(col(\"_c0\") != 'fake')\n",
    "        .select('_c0'),\n",
    "    on='_c0',\n",
    "    how='inner'\n",
    ")\n",
    "gt_counted = ground_truth.count()\n",
    "\n",
    "# Convert from_parties to a Spark SQL IN clause string\n",
    "from_parties_str = ','.join(str(p) for p in from_parties)\n",
    "\n",
    "flattened_df = multiparty_struct.filter(\n",
    "    expr(f\"\"\"\n",
    "        exists(records, x -> x.origin = {to_party})\n",
    "        AND exists(records, x -> x.origin IN ({from_parties_str}))\n",
    "    \"\"\")\n",
    ").select(\n",
    "    explode(col(\"records\")).alias(\"record\")\n",
    ").select(\n",
    "    col(\"record.origin\").alias(\"origin\"),\n",
    "    col(\"record.id\").alias(\"id\")\n",
    ")\n",
    "\n",
    "filtered_df = flattened_df.dropDuplicates([\"origin\", \"id\"])\n",
    "\n",
    "results = []\n",
    "for i in (0,1,2,3,4):\n",
    "    # Assign the correct origin value to each dataset\n",
    "    df_with_origin = processed_dfs[i].withColumn(\"origin\", lit(i))\n",
    "\n",
    "    # Filter only the relevant (origin, id) pairs for this dataset\n",
    "    df_filtered = filtered_df.filter(col(\"origin\") == (i))\n",
    "\n",
    "    # Join to get _c0\n",
    "    joined_df = df_with_origin.join(\n",
    "        df_filtered,\n",
    "        on=[\"origin\", \"id\"],\n",
    "        how=\"inner\"\n",
    "    ).select(\"origin\", \"id\", \"_c0\")\n",
    "\n",
    "    results.append(joined_df)\n",
    "\n",
    "final_result = reduce(lambda df1, df2: df1.unionByName(df2), results)\n",
    "if DEBUG : final_result.show()\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2f5821-b9b9-48ab-8569-8b53da34ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth (GT): 1000\n",
      "True Positives (TP): 934\n",
      "False Positives (FP): 7\n",
      "False Negatives (FN): 66\n",
      "Total Predictions: 941\n",
      "Precision: 0.9926\n",
      "Recall: 0.9340\n",
      "F1 Score: 0.9624\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================================\n",
    "# 5. This is for the develpment stage to calcualte Statistics. It shouldnnt interfere with the \n",
    "# code in the production\n",
    "\n",
    "filtered = final_result.join(\n",
    "    ground_truth.distinct().select(\"_c0\"),\n",
    "    on=\"_c0\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "total = final_result.count()\n",
    "\n",
    "gt = gt_counted\n",
    "tp = filtered.select(\"_c0\").count()\n",
    "fp = total - tp\n",
    "fn = gt - tp\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "print(f\"Ground Truth (GT): {gt}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Total Predictions: {total}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "\n",
    "# metrics are aslo very important. To visualize the struct\n",
    "# pass_block_counts = multiparty_struct.groupBy(\"pass_id\").count().orderBy(\"pass_id\")\n",
    "# pass_block_counts.show(truncate=False)\n",
    "\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2d66e-bdb4-4d1b-961f-a52aab5ba705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
