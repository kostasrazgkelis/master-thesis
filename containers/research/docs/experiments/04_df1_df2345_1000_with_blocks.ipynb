{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b5e2fb-7c82-4ed6-aa80-82471d5ec58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (356 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.9/356.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jellyfish\n",
      "Successfully installed jellyfish-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a03b091-79cd-40f1-9007-152b770e3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "DEV_ENV = \"spark.local.conf\"\n",
    "# dev_evn = \"spark.production.conf\"\n",
    "CONFIG_PATH = os.path.join(project_root, \"packages\", \"conf\", DEV_ENV)\n",
    "OUTPUT_PATH = os.path.join(project_root, \"output\", \"structs\")\n",
    "\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "from functools import reduce\n",
    "\n",
    "from itertools import combinations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, lit, col, concat_ws, struct, collect_list, expr, explode, monotonically_increasing_id, count, when, broadcast, size,  avg, stddev\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, VarcharType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from packages.utils.dataset_generation import MyDatasets\n",
    "from packages.utils.spark_udfs import soundex_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9a1274-b622-4eaf-96b0-3a0d6b218cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 1. Create a Spark session\n",
    "\n",
    "spark_builder = SparkSession.builder.master(\"local[*]\").appName(\"LocalTesting\")\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            spark_builder = spark_builder.config(key.strip(), value.strip())\n",
    "\n",
    "spark = spark_builder.getOrCreate()\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298a7dcd-f26e-497f-8bb2-0816bf3b576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = MyDatasets()\n",
    "dfs = datasets.size_1000().exclude([\"0\"]).soundex().noise_10().hash_sha256()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a03d395-5548-4309-babe-1a754b617a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].to_csv(\"1000_10_A.csv\")\n",
    "dfs[1].to_csv(\"1000_10_B.csv\")\n",
    "dfs[2].to_csv(\"1000_10_C.csv\")\n",
    "dfs[3].to_csv(\"1000_10_D.csv\")\n",
    "dfs[4].to_csv(\"1000_10_E.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a702a0-7992-4532-b835-a94e878a705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "COLUMNS = [\"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\"]\n",
    "datasets = MyDatasets()\n",
    "# dfs = datasets.size_1000().exclude([\"0\"]).soundex()\n",
    "# dfs = datasets.size_1000().exclude([\"0\"]).soundex().noise_50().hash_sha256()\n",
    "# dfs = datasets.size_10000()\n",
    "# dfs = datasets.size_50000()\n",
    "# dfs = datasets.size_75000()\n",
    "# dfs = datasets.size_100000().exclude([\"0\"]).soundex().noise_200().hash_sha256() # full dataset size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b5eb91-3fd0-43b9-8c1d-30a521fa2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 2. Load data and preprocess them\n",
    "processed_dfs = []\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df = df[[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]]\n",
    "    df.columns = [\"_c0\", *COLUMNS]\n",
    "\n",
    "    df_tmp = spark.createDataFrame(df)\n",
    "    df_tmp = df_tmp.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_tmp = df_tmp.withColumn(\"origin\", lit(i))\n",
    "\n",
    "    # for c in COLUMNS:\n",
    "    #     df_tmp = df_tmp.withColumn(c, soundex_udf(df_tmp[c]))\n",
    "\n",
    "    processed_dfs.append(df_tmp.select('origin', 'id', '_c0', *COLUMNS))\n",
    "\n",
    "\n",
    "multiparty_datasets = processed_dfs[0]\n",
    "for df in processed_dfs[1:]:\n",
    "    multiparty_datasets = multiparty_datasets.union(df)\n",
    "\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a697bd5-ced2-4979-aa28-92555a6c96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 3. Create the struct - The struct can be used for any combination of data requests we give\n",
    "column_combinations = list(combinations(COLUMNS, 3))\n",
    "\n",
    "blocking_passes = []\n",
    "for pass_id, combo in enumerate(column_combinations):\n",
    "\n",
    "    df_with_block_key = multiparty_datasets.withColumn(\n",
    "        \"block_key\",\n",
    "        concat_ws(\"_\", *[col(c) for c in combo])\n",
    "    )\n",
    "\n",
    "    blocked_df = df_with_block_key.groupBy(\"block_key\").agg(\n",
    "        collect_list(struct(*['origin', 'id'])).alias(\"records\")\n",
    "    ).withColumn(\"pass_id\", lit(pass_id))\n",
    "\n",
    "    blocked_df = blocked_df.filter(size(col(\"records\")) > 1)\n",
    "    blocking_passes.append(blocked_df)\n",
    "\n",
    "multiparty_struct = reduce(lambda df1, df2: df1.unionByName(df2), blocking_passes)\n",
    "multiparty_struct = multiparty_struct.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "multiparty_struct.count()\n",
    "\n",
    "if DEBUG : multiparty_struct.show(truncate=False)\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c142bcda-1895-4f7c-8d58-11e674b9567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|pass_id|count|\n",
      "+-------+-----+\n",
      "|0      |28726|\n",
      "|1      |21735|\n",
      "|2      |37717|\n",
      "|3      |24194|\n",
      "|4      |40636|\n",
      "|5      |96293|\n",
      "|6      |27591|\n",
      "|7      |49822|\n",
      "|8      |46654|\n",
      "|9      |48149|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+------------------+------------------+----------------+\n",
      "|pass_id|average_block_size|stddev_block_size |number_of_blocks|\n",
      "+-------+------------------+------------------+----------------+\n",
      "|0      |2.28702220984474  |0.6318635837788373|28726           |\n",
      "|1      |2.3437313089487004|0.6190506253664175|21735           |\n",
      "|2      |2.43351804226211  |0.9492441424139563|37717           |\n",
      "|3      |2.330577829213855 |0.6174697571559731|24194           |\n",
      "|4      |2.4723151885028054|1.0820396294686985|40636           |\n",
      "|5      |2.4953942654190855|1.0434780792407468|96293           |\n",
      "|6      |2.3637055561596174|0.6784951993596732|27591           |\n",
      "|7      |3.00764722411786  |2.6086555787640253|49822           |\n",
      "|8      |2.565010502850774 |1.1999464397700417|46654           |\n",
      "|9      |2.6549045670730442|1.4558802846189296|48149           |\n",
      "+-------+------------------+------------------+----------------+\n",
      "\n",
      "+--------------------------+-------------------------+----------------------+\n",
      "|overall_average_block_size|overall_stddev_block_size|total_possible_matches|\n",
      "+--------------------------+-------------------------+----------------------+\n",
      "|2.5340045597211973        |1.3368677208167523       |421517                |\n",
      "+--------------------------+-------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Per-pass block metrics\n",
    "pass_block_counts = multiparty_struct.groupBy(\"pass_id\").count().orderBy(\"pass_id\")\n",
    "pass_block_counts.show(truncate=False)\n",
    "\n",
    "block_size_metrics = multiparty_struct.withColumn(\"block_size\", size(col(\"records\"))) \\\n",
    "    .groupBy(\"pass_id\") \\\n",
    "    .agg(\n",
    "        avg(\"block_size\").alias(\"average_block_size\"),\n",
    "        stddev(\"block_size\").alias(\"stddev_block_size\"),\n",
    "        count(\"*\").alias(\"number_of_blocks\")\n",
    "    ) \\\n",
    "    .orderBy(\"pass_id\")\n",
    "\n",
    "block_size_metrics.show(truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# Overall block metrics across all passes\n",
    "overall_block_metrics = multiparty_struct.withColumn(\"block_size\", size(col(\"records\"))) \\\n",
    "    .agg(\n",
    "        avg(\"block_size\").alias(\"overall_average_block_size\"),\n",
    "        stddev(\"block_size\").alias(\"overall_stddev_block_size\"),\n",
    "        count(\"*\").alias(\"total_possible_matches\")\n",
    "    )\n",
    "\n",
    "overall_block_metrics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bba772-a894-4af4-a448-3034f124eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# 4. We have created our struct, we need to get the data now and we need to set the experiment.\n",
    "# If we want to get the matches of party 0 with all the rest of the matches we need to see \n",
    "# the configuration like this example\n",
    "\n",
    "\n",
    "# multiparty_struct = spark.read.parquet(os.path.join(OUTPUT_PATH, \"multiparty_struct\"))\n",
    "\n",
    "to_party = 0\n",
    "from_parties = (1, 2, 3, 4)\n",
    "\n",
    "party = processed_dfs[to_party]\n",
    "\n",
    "# Union only the specified from_parties datasets\n",
    "ground_truth_datasets = processed_dfs[from_parties[0]]\n",
    "for i in from_parties[1:]:\n",
    "    ground_truth_datasets = ground_truth_datasets.union(processed_dfs[i])\n",
    "\n",
    "# Perform join and count\n",
    "ground_truth = party.select('_c0').join(\n",
    "    ground_truth_datasets\n",
    "        .filter(col(\"_c0\") != 'fake')\n",
    "        .select('_c0'),\n",
    "    on='_c0',\n",
    "    how='inner'\n",
    ")\n",
    "gt_counted = ground_truth.count()\n",
    "\n",
    "# Convert from_parties to a Spark SQL IN clause string\n",
    "from_parties_str = ','.join(str(p) for p in from_parties)\n",
    "\n",
    "flattened_df = multiparty_struct.filter(\n",
    "    expr(f\"\"\"\n",
    "        exists(records, x -> x.origin = {to_party})\n",
    "        AND exists(records, x -> x.origin IN ({from_parties_str}))\n",
    "    \"\"\")\n",
    ").select(\n",
    "    explode(col(\"records\")).alias(\"record\")\n",
    ").select(\n",
    "    col(\"record.origin\").alias(\"origin\"),\n",
    "    col(\"record.id\").alias(\"id\")\n",
    ")\n",
    "\n",
    "filtered_df = flattened_df.dropDuplicates([\"origin\", \"id\"])\n",
    "\n",
    "results = []\n",
    "for i in (0,1,2,3,4):\n",
    "    # Assign the correct origin value to each dataset\n",
    "    df_with_origin = processed_dfs[i].withColumn(\"origin\", lit(i))\n",
    "\n",
    "    # Filter only the relevant (origin, id) pairs for this dataset\n",
    "    df_filtered = filtered_df.filter(col(\"origin\") == (i))\n",
    "\n",
    "    # Join to get _c0\n",
    "    joined_df = df_with_origin.join(\n",
    "        df_filtered,\n",
    "        on=[\"origin\", \"id\"],\n",
    "        how=\"inner\"\n",
    "    ).select(\"origin\", \"id\", \"_c0\")\n",
    "\n",
    "    results.append(joined_df)\n",
    "\n",
    "final_result = reduce(lambda df1, df2: df1.unionByName(df2), results)\n",
    "\n",
    "multiparty_datasets.unpersist()\n",
    "\n",
    "if DEBUG : final_result.show()\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2f5821-b9b9-48ab-8569-8b53da34ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth (GT): 100000\n",
      "True Positives (TP): 96421\n",
      "False Positives (FP): 48010\n",
      "False Negatives (FN): 3579\n",
      "Total Predictions: 144431\n",
      "Precision: 0.6676\n",
      "Recall: 0.9642\n",
      "F1 Score: 0.7889\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================================\n",
    "# 5. This is for the develpment stage to calcualte Statistics. It shouldnnt interfere with the \n",
    "# code in the production\n",
    "\n",
    "filtered = final_result.join(\n",
    "    ground_truth.distinct().select(\"_c0\"),\n",
    "    on=\"_c0\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "total = final_result.count()\n",
    "\n",
    "gt = gt_counted\n",
    "tp = filtered.select(\"_c0\").count()\n",
    "fp = total - tp\n",
    "fn = gt - tp\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "print(f\"Ground Truth (GT): {gt}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Total Predictions: {total}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "# ==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1db7ae-d7cc-4594-b266-10ff0fc776a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
