{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e153edb-1ec0-4164-9dd6-374b4c8375f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and sampling datasets...\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n",
    "\n",
    "# DF1 vs DF2 Sample Experiment (Seed 42)\n",
    "\n",
    "# This experiment tests the entity matching pipeline using a 10% sample from df1 and df2 datasets with a fixed random seed for reproducibility.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Setup paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "from packages.pyspark.entity_matching_pipeline import run_entity_matching\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DF1vsDF2SampleSeed42\").getOrCreate()\n",
    "\n",
    "# Load and sample datasets with fixed seed for reproducibility\n",
    "print(\"Loading and sampling datasets...\")\n",
    "\n",
    "# Load 10% sample from df1 and df2 with seed 42\n",
    "df1_pandas = pd.read_csv(os.path.join(project_root, \"data/df1.csv\"), \n",
    "                        usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "\n",
    "df2_pandas = pd.read_csv(os.path.join(project_root, \"data/df2.csv\"), \n",
    "                        usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2e52ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 sample size: 10000\n",
      "df2 sample size: 10000\n",
      "Expected results: {'gt': 2531, 'tp': 1775, 'fp': 92, 'fn': 756}\n",
      "Running entity matching pipeline...\n",
      "Starting Multi-Dataset Entity Matching Pipeline...\n",
      "Processing 1 left dataset vs 1 right datasets\n",
      "Step 1: Preprocessing left dataframe...\n",
      "Step 1b: Preprocessing and union right dataframes...\n",
      "Unified right dataset has 10000 total records\n",
      "Step 2: Creating entity keys...\n",
      "Step 3: Calculating similarity matrix...\n",
      "Step 4: Applying similarity threshold...\n",
      "Step 5: Assigning entities to buckets...\n",
      "Step 6: Calculating ground truth...\n",
      "Step 7: Evaluating results...\n",
      "Multi-dataset pipeline completed successfully!\n",
      "\n",
      "Bucket Summary:\n",
      "Total buckets: 1867\n",
      "+---------+-----------+------------------+\n",
      "|bucket_id|bucket_size|    avg_similarity|\n",
      "+---------+-----------+------------------+\n",
      "| AA100187|          1|               0.6|\n",
      "| AA100252|          1|               0.6|\n",
      "| AA100290|          1|               0.8|\n",
      "| AA100327|          1|               0.6|\n",
      "| AA100360|          2|               0.7|\n",
      "| AA100381|          1|               0.8|\n",
      "| AA100422|          1|               0.8|\n",
      "|  AA10048|          3|0.7333333333333334|\n",
      "| AA100508|          1|               0.8|\n",
      "| AA100514|          1|               0.6|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Evaluation Metrics:\n",
      "ground_truth: 2531\n",
      "true_positives: 1775\n",
      "false_positives: 92\n",
      "false_negatives: 756\n",
      "precision: 0.9507230851633637\n",
      "recall: 0.7013038324772817\n",
      "f1_score: 0.8071850841291496\n",
      "total_buckets: 1867\n",
      "average_bucket_size: 1.153722549544724\n",
      "\n",
      "Comparison with Expected Results:\n",
      "Expected Ground Truth: 2531, Actual: 2531\n",
      "Expected True Positives: 1775, Actual: 1775\n",
      "Expected False Positives: 92, Actual: 92\n",
      "Expected False Negatives: 756, Actual: 756\n",
      "\n",
      "Precision: 0.9507\n",
      "Recall: 0.7013\n",
      "F1-Score: 0.8072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to Spark DataFrames\n",
    "df1 = spark.createDataFrame(df1_pandas)\n",
    "df2 = spark.createDataFrame(df2_pandas)\n",
    "\n",
    "print(f\"df1 sample size: {df1.count()}\")\n",
    "print(f\"df2 sample size: {df2.count()}\")\n",
    "\n",
    "# Expected results from previous analysis\n",
    "expected_results = {\"gt\": 2531, \"tp\": 1775, \"fp\": 92, \"fn\": 756}\n",
    "print(f\"Expected results: {expected_results}\")\n",
    "\n",
    "\n",
    "# Run entity matching pipeline\n",
    "print(\"Running entity matching pipeline...\")\n",
    "buckets, metrics = run_entity_matching(\n",
    "    spark=spark,\n",
    "    left_df=df1,\n",
    "    right_dataframes=[df2],\n",
    "    similarity_threshold=0.6,\n",
    "    min_matching_columns=3\n",
    ")\n",
    "\n",
    "print(\"\\nBucket Summary:\")\n",
    "print(f\"Total buckets: {buckets.count()}\")\n",
    "buckets.select(\"bucket_id\", \"bucket_size\", \"avg_similarity\").show(10)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "    \n",
    "# Compare with expected results\n",
    "print(\"\\nComparison with Expected Results:\")\n",
    "print(f\"Expected Ground Truth: {expected_results['gt']}, Actual: {metrics['ground_truth']}\")\n",
    "print(f\"Expected True Positives: {expected_results['tp']}, Actual: {metrics['true_positives']}\")\n",
    "print(f\"Expected False Positives: {expected_results['fp']}, Actual: {metrics['false_positives']}\")\n",
    "print(f\"Expected False Negatives: {expected_results['fn']}, Actual: {metrics['false_negatives']}\")\n",
    "\n",
    "print(f\"\\nPrecision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6b3da-cae6-4bbe-9c3c-be8f90db0b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df1_pandas).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a65211-ea99-4b08-94dc-0be6d3e3c8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
